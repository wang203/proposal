\documentclass{iuphd_proposal}
\usepackage[utf8]{inputenc}
\usepackage{cite}

% exta packages
\usepackage{graphicx, caption, subfig, setspace, soul, color, paralist}
% \usepackage{subcaption}

\usepackage{rotating}
\usepackage{relsize}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{multirow}
\usetikzlibrary{arrows,shapes}

% extra commands
\captionsetup[figure]{font={stretch=1.1}}
\newcommand{\fixme}[1]{\sethlcolor{yellow}\hl{#1}}
\newcommand{\xhdr}[1]{\vspace{4pt}\noindent {\textit{\textbf{#1.}}}}
%\newcommand{\rebuttal}[1]{\textcolor{red}{#1}}
\newcommand{\rebuttal}[1]{#1}

%data for Title Page
\title{}
\author{}
\date{}

%data for Acceptance Page
\committeechair{}
\readertwo{}
\readerthree{}
\readerfour{}
\defensedate{}


%data for Copyright Page
\cryear{}

\begin{document}
\maketitle

% \acceptancepage

% \copyrightpage

%\begin{dedication}
%\end{dedication}

% \begin{acknowledgments}
% \end{acknowledgments}

%\begin{preface}
%\end{preface}

\begin{abstract}
% Egocentric cameras aim to approximate a person's field of view, which provides insight into how people interact with the world. Consequently, many cognitive researchers are interested in using wearable camera systems as tools to study attention, perception, and learning. These systems typically capture vast amounts of image data, so to fully harness the potential of this novel observational paradigm, sophisticated techniques to automatically annotate and understand the data are needed. However, analyzing first-person imagery introduces many unique challenges, as it is usually recorded passively
% %or unintentionally
% without artistic intent and therefore lacks many of the clean characteristics of traditional photography.
% 
% This thesis presents novel computer vision approaches to automatically analyze first-person imaging data. The focus of these approaches lies in extracting and understanding hands in the egocentric field of view. Hands are almost omnipresent and constitute our primary channel of interaction with the physical world. To that end, we argue that analyzing hands is an important factor towards the goal of automatically understanding human behavior from egocentric images and videos.
% We propose three different approaches that aim to extract meaningful and useful information about hands in the context of social interactions. First, we consider laboratory videos of joint toy play between infants and parents, and develop a method to track and, importantly, distinguish hands based on spatial constraints imposed by the egocentric paradigm. This method allows us collect fine-grained hand appearance statistics that contribute new evidence towards how infants and their parents coordinate attention through eye-hand coordination.
% Next, we build upon this approach to develop a generalized, probabilistic framework that jointly models temporal and spatial biases of hand locations. We demonstrate that this approach achieves notable results in disambiguating hands even when combined with noisy initial detections that may occur in naturalistic videos.
% Finally, we ask to what extent we can identify hand types and poses directly based on visual appearances. We collect a large-scale egocentric video dataset with pixel-level hand annotations to permit the training of data-driven recognition models like convolutional neural networks. Results indicate that not only can we distinguish hands, but also infer activities from hand poses.
\end{abstract}

\tableofcontents

\chapter{Introduction}
\input{intro}

% \chapter{Analyzing Hands in Infants' Egocentric Views}
% Bla Bla Bla
% 
% \chapter{A Probabilistic Framework to Distinguish Hands}
% Bla Bla Bla
% 
% \chapter{Detecting Hands based on Visual Appearance}
% Bla Bla Bla
% 
% \chapter{Using Hands to Infer Social Activities}
% Bla Bla Bla
% 
% \chapter{Conclusion}
% \input{conclusion}
% 
% \chapter{Dissertation Timeline}
% \input{timeline}
% 
% \chapter*{Curriculum Vitae}
% \input{cv}


\bibliographystyle{plain}
\bibliography{proposal_short}

%\include{CV}
\end{document}
