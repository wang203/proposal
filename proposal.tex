\documentclass{iuphd_proposal}
\usepackage[utf8]{inputenc}
\usepackage{cite}

% exta packages
\usepackage{graphicx, caption, subfig, setspace, soul, color, paralist}
% \usepackage{subcaption}

\usepackage{rotating}
\usepackage{relsize}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{multirow}
\usetikzlibrary{arrows,shapes}

% extra commands
\captionsetup[figure]{font={stretch=1.1}}
\newcommand{\fixme}[1]{\sethlcolor{yellow}\hl{#1}}
\newcommand{\xhdr}[1]{\vspace{4pt}\noindent {\textit{\textbf{#1.}}}}
%\newcommand{\rebuttal}[1]{\textcolor{red}{#1}}
\newcommand{\rebuttal}[1]{#1}

%data for Title Page
\title{Web-scale Visual Content Mining with Computer Vision Methods}
\author{}
\date{}

%data for Acceptance Page
\committeechair{}
\readertwo{}
\readerthree{}
\readerfour{}
\defensedate{}


%data for Copyright Page
\cryear{}

\begin{document}
\maketitle

% \acceptancepage

% \copyrightpage

%\begin{dedication}
%\end{dedication}

% \begin{acknowledgments}
% \end{acknowledgments}

%\begin{preface}
%\end{preface}

\begin{abstract}
Social media provides low-cost and easy access to a large number of images with a wide variety of visual content. With this new source of large-scale image data, there is a critical demand for novel, scalable methods that can understand the information behind the visual content. For example, photos shared on social media are usually accompanied with temporal and location metadata, user profiles and textual annotations. When we integrate the visual content by user profiles or location tags, we can estimate a large variety of information. For example, we can help research in Psychology when we are interested in the human beings who take these photos and in Ecology when we are interested in the natural scenes as the visual content. 
These attractive properties of this new data source also bring us new challenges.
Being able to efficiently processing data with advanced computer vision techniques is an important requirement.
To accommodate the graphical structure of metadata in social media, we also need to develop new models and systems.

This thesis introduces three projects: a cloud based robotic system testing the feasibility of a memory and computing consuming task to work in near real time; a data mining application supporting Psychology research in gender differences in preference of color; and a natural event tracking system on a continental scale.
We design and implement a cloud based system to apply these algorithms for object detection and recognition with the best precision in near real time. We test our method on an aerial autonomous vehicle.
We take advantage of this new data source for psychology research. Based on the color pixels of these photos, we analyze the gender difference in color preference. We study the distribution of color pixels with respect to genders of photographers across geographic locations and content. We find strong sex differences for the predominant reddish and bluish hues, with female users uploading more photographs containing more reddish pixels and male users uploading more photographs containing more bluish pixels.  Furthermore, we take Google Street View to represent the color distribution in the environment, and compare with Flickr photos in three popular outdoor locations. We observe the overall preference of saturated color and reddish color of human compared to the environment.
By mining visual content on social media websites web-scale images further benefit studies about the state of nature. We develop and evaluate three models to study ecology phenomena such as snow and vegetation coverage.
First, we learn a binomial distribution to model the probability of the appearance of a natural phenomenon. 
Then, we compute the histogram of the confidence of each image being a positive evidence. Based on this histogram, we learn a classification model for the confidence of each user, and similarly apply this method to predict for each time and location.
Finally, inspired by the study of multiple instance learning, we propose an end to end system taking a sequence of images as input and giving predictions as the final output of this holistic system. This process will happen twice, once for users and once for each day and location. The two aggregating processes are optimized simultaneously.


% Egocentric cameras aim to approximate a person's field of view, which provides insight into how people interact with the world. Consequently, many cognitive researchers are interested in using wearable camera systems as tools to study attention, perception, and learning. These systems typically capture vast amounts of image data, so to fully harness the potential of this novel observational paradigm, sophisticated techniques to automatically annotate and understand the data are needed. However, analyzing first-person imagery introduces many unique challenges, as it is usually recorded passively
% %or unintentionally
% without artistic intent and therefore lacks many of the clean characteristics of traditional photography.
% 
% This thesis presents novel computer vision approaches to automatically analyze first-person imaging data. The focus of these approaches lies in extracting and understanding hands in the egocentric field of view. Hands are almost omnipresent and constitute our primary channel of interaction with the physical world. To that end, we argue that analyzing hands is an important factor towards the goal of automatically understanding human behavior from egocentric images and videos.
% We propose three different approaches that aim to extract meaningful and useful information about hands in the context of social interactions. First, we consider laboratory videos of joint toy play between infants and parents, and develop a method to track and, importantly, distinguish hands based on spatial constraints imposed by the egocentric paradigm. This method allows us collect fine-grained hand appearance statistics that contribute new evidence towards how infants and their parents coordinate attention through eye-hand coordination.
% Next, we build upon this approach to develop a generalized, probabilistic framework that jointly models temporal and spatial biases of hand locations. We demonstrate that this approach achieves notable results in disambiguating hands even when combined with noisy initial detections that may occur in naturalistic videos.
% Finally, we ask to what extent we can identify hand types and poses directly based on visual appearances. We collect a large-scale egocentric video dataset with pixel-level hand annotations to permit the training of data-driven recognition models like convolutional neural networks. Results indicate that not only can we distinguish hands, but also infer activities from hand poses.
\end{abstract}

\tableofcontents

\chapter{Introduction}
\input{intro}

% \chapter{Analyzing Hands in Infants' Egocentric Views}
% Bla Bla Bla
% 
% \chapter{A Probabilistic Framework to Distinguish Hands}
% Bla Bla Bla
% 
% \chapter{Detecting Hands based on Visual Appearance}
% Bla Bla Bla
% 
% \chapter{Using Hands to Infer Social Activities}
% Bla Bla Bla
% 
% \chapter{Conclusion}
% \input{conclusion}
% 
% \chapter{Dissertation Timeline}
% \input{timeline}
% 
% \chapter*{Curriculum Vitae}
% \input{cv}


\bibliographystyle{plain}
\bibliography{proposal_short}

%\include{CV}
\end{document}
